"""Attention Interface to handle various attention operators and cache operations.

This module provides an interface between the high-level runtime and cache management system and
the low-level functional attention operators. The interface is designed to provide a homogeneous
object-oriented interface to the high-level runtime via the SequenceInfo dataclass. The SequenceInfo
is also responsible for functionalizing information about the sequence and pass it on the the
various attention interface. The AttentionDescriptor is the main interface to the attention operator
and operates on a purely functional paradigm that is compatible with the torch custom op system.

"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Callable, Dict, List, Literal, Optional, Protocol, Sequence, Tuple, Type, Union

import torch
from torch._ops import OpOverloadPacket
from torch.export import Dim
from torch.fx import Node

DynamicShape = Dict[int, Dim]  # indicating the dynamic shape in tensor dimension
DynamicShapeCallback = Callable[[], DynamicShape]


@dataclass
class CacheConfig:
    """A dataclass to hold information how to configure the cache."""

    dtype: Optional[torch.dtype] = None


@dataclass
class SequenceInfo:
    """A dataclass to hold information about how the sequence is laid out and stored in cache.

    We assume the sequence + cache is laid out in the following way. Also note that we differentiate
    between arguments that are originally part of the model/graph and arguments that are needed for
    the attention operator when we switch to cached+flattened attention.

    # ORIGINAL MODEL ARGUMENTS #####################################################################
    - input_ids: [id_0, ..., id_{s_total-1}]
      flattened sequence of [b, 1] or [1, s_total]. We use [b, 1] to denote generate-only batches.
    - position_ids: [pos_0, ..., pos_{s_total-1}]
      flattened sequence of [b, 1] or [1, s_total] indicating absolute position ids for every token
      in the input_ids sequence. We use [b, 1] to denote generate-only batches.

    NOTE: ``input_ids`` and ``position_ids`` are initially expected to be of shape [b, seq_len]
    before we switch to cached+flattened attention.

    # EXTRA ARGUMENTS NEEDED FOR ATTENTION OPERATORS FOR FLATTENED SEQUENCES + CACHES ##############
    - seq_len: [s_0, s_1, ..., s_{b-1}] such that s_total = sum(s_i)
      Describes how long each sequence is. For example,
      input_ids[:s_0] will correspond to sequence 0 in the batch and input_ids[s_0:s_1] will
      correspond to sequence 1 in the batch.
    - input_pos: [pos_0, ..., pos_{b-1}]
      Corresponds to the total number of tokens that has been already been cached for each sequence
      in the batch.
    - cache_loc: [c0, ...., c_{np-1}] where np is total number of pages allocated to describe all
      sequences in the batch.
    - pages_per_seq: [ps_0, ps_1, ..., ps_{b-1}] where ps_i is the number of pages allocated for
      sequence i. Note that, for example, cache_loc[p_0:p_1] will correspond to the pages associated
      with sequence 1 in the batch.

    ################################################################################################

    Here are a couple of notes to emphasize this notation:

    - The total number of allocated token space for sequence i is given by ps_i * page_size. This is
      the total number of tokens that can be cached for each sequence.

    - NOTE: It must hold that pos_i + s_i <= ps_i * page_size for all i in [0, b-1]. Moreover, it is
      the responsibility of the cache manager and/or runtime to ensure sufficient page allocation
      for each sequence.

    """

    ## USE TO INITIALIZE DATA CLASS  ###############################################################
    # max_seq_len corresponds the maximum number of tokens in any sequence. It includes the tokens in the
    # input sequence and the tokens generated by the model.
    max_seq_len: int = 1
    # max_batch_size corresponds to the maximum number of sequences (or requests) that the model can process.
    max_batch_size: int = 1
    # page_size is the granularity with which the cache pages are allocated for a paged kv cache.
    # For an unpaged cache, the page size should be set to max_seq_len.
    # Also note that two sequences in a batch can not share a page.
    page_size: int = 0
    # max_num_tokens is the maximum number of tokens that the model can process across all sequences in the batch.
    # If a batch is composed of context-only requests of input sequence length ISL,
    # then the maximum number of sequences possible in the batch is min (max_batch_size, max_num_tokens // ISL).
    # Similarly, if a batch is composed of generate-only requests,
    # then the maximum number of sequences possible in the batch is min (max_batch_size, max_num_tokens).
    max_num_tokens: Optional[int] = None

    ## PRIVATE FIELDS ##############################################################################
    _sequence_lengths: List[int] = field(default_factory=list)
    _num_pages: int = 1

    def __post_init__(self):
        if self.page_size < 1:
            self.page_size = self.max_seq_len

        # NOTE (lucaslie): WAR to address issue when using flashinfer attention with
        # (max_batch_size, max_seq_len) input in trtllm runtime.
        # see https://github.com/NVIDIA/TensorRT-LLM/issues/4504
        max_seq_len_adjusted = self.max_seq_len + 1

        if self.max_num_tokens is None or self.max_num_tokens < 1:
            self.max_num_tokens = self.max_batch_size * max_seq_len_adjusted
        # if the provided max_num_tokens is less than the max_batch_size * max_seq_len,
        # we use the provided max_num_tokens to calculate the number of pages
        total_tokens = min(self.max_num_tokens, self.max_batch_size * max_seq_len_adjusted)
        # Num pages can not be less than max_batch_size.
        self._num_pages = max(
            self.max_batch_size,
            (total_tokens) // self.page_size + (total_tokens % self.page_size > 0),
        )
        # sanity check
        assert self.num_pages >= self.max_batch_size, "num_pages can't be less than max_batch_size"

        # keep a list-like object of sequence lengths for simplicity as well
        self._sequence_lengths = [0] * self.max_batch_size

        # indicator if extra args are activated that are needed for cached attention backends
        self._is_cached_attn = False

        ### TENSOR FIELDS/ARGS FOR UNCACHED AND CACHED ATTENTION ###################################
        # UNCACHED TENSOR FIELDS
        self.input_ids = torch.ones(self.max_batch_size, 1, dtype=torch.int)
        self.position_ids = torch.zeros(self.max_batch_size, 1, dtype=torch.long)
        self._uncached_arg_names = ["input_ids", "position_ids"]

        # CACHED TENSOR FIELDS (for cached attention backends)
        self.seq_len = torch.empty(self.max_batch_size, dtype=torch.int)
        self.input_pos = torch.empty_like(self.seq_len)
        self.cache_loc = torch.empty(self.num_pages, dtype=torch.int)
        self.pages_per_seq = torch.empty_like(self.seq_len)
        self._cached_arg_names = ["seq_len", "input_pos", "cache_loc", "pages_per_seq"]

        # DYNAMIC SHAPES
        # --> initialized lazily since Dim is not picklable for multi-processing
        self._uncached_dynamic_shapes: Optional[Dict[str, DynamicShape]] = None
        self._cached_dynamic_shapes: Optional[Dict[str, DynamicShape]] = None
        ############################################################################################

        ### EXTRA ARGS #############################################################################
        self._extra_args: Dict[str, torch.Tensor] = {}
        self._extra_dynamic_shapes: Optional[Dict[str, DynamicShape]] = None
        self._extra_dynamic_shapes_callbacks: Dict[str, DynamicShapeCallback] = {}
        ############################################################################################

        # call reset once to initialize the tensors
        self.reset()

    @property
    def device(self) -> torch.device:
        return self.input_pos.device

    def _named_args(
        self, include_extra_args: bool = True, include_cached_args: bool = True
    ) -> Dict[str, torch.Tensor]:
        args: Dict[str, torch.Tensor] = {}
        for name in self._uncached_arg_names:
            args[name] = getattr(self, name)

        if include_extra_args:
            args.update(self._extra_args)

        if include_cached_args:
            for name in self._cached_arg_names:
                args[name] = getattr(self, name)

        return args

    @property
    def named_args(self) -> Dict[str, torch.Tensor]:
        """Return a dictionary of named arguments."""
        return self._named_args(include_extra_args=True, include_cached_args=self._is_cached_attn)

    @property
    def named_standard_args(self) -> Dict[str, torch.Tensor]:
        """Return a dictionary of named standard arguments."""
        return self._named_args(include_extra_args=False, include_cached_args=self._is_cached_attn)

    @property
    def args(self) -> Tuple[torch.Tensor, ...]:
        """Return a tuple of arguments."""
        return tuple(self.named_args.values())

    @property
    def extra_args_for_prepare_metadata(self) -> Tuple:
        """Return a tuple of extra (const, non-tensor) arguments for the prepare_metadata op."""
        return (self.page_size,)

    @property
    def named_dynamic_shapes(self) -> Dict[str, Dict[str, Dim]]:
        """Return dynamic shapes of sequence info tensors.

        NOTE: will be lazily initialized since the Dim object is not picklable for multi-processing.
        """
        # lazy initialization of dynamic shapes with Dim objects
        if self._uncached_dynamic_shapes is None:
            # set up shape for uncached args (same for all, i.e., batch_size and seq_len)
            bs_seq_len_shape: DynamicShape = {}
            if self.max_batch_size > 1:
                bs_seq_len_shape[0] = Dim("batch_size", max=self.max_batch_size)
            bs_seq_len_shape[1] = Dim("seq_len", max=self.max_seq_len)
            self._uncached_dynamic_shapes = {k: bs_seq_len_shape for k in self._uncached_arg_names}

        named_dynamic_shapes = self._uncached_dynamic_shapes.copy()

        # add dynamic shapes for extra args
        if self._extra_dynamic_shapes is None:
            self._extra_dynamic_shapes = {
                k: callback() for k, callback in self._extra_dynamic_shapes_callbacks.items()
            }
        named_dynamic_shapes.update(self._extra_dynamic_shapes)

        # fixed shape for remaining cached attention args
        if self._is_cached_attn:
            if self._cached_dynamic_shapes is None:
                self._cached_dynamic_shapes = {k: {} for k in self._cached_arg_names}
            named_dynamic_shapes.update(self._cached_dynamic_shapes)

        return named_dynamic_shapes

    @property
    def dynamic_shapes(self) -> Tuple[DynamicShape, ...]:
        """Return dynamic shapes of sequence info tensors."""
        return tuple(self.named_dynamic_shapes.values())

    @property
    def num_sequences(self) -> int:
        return len(self._sequence_lengths)

    @property
    def sequence_lengths(self) -> List[int]:
        return self._sequence_lengths

    @property
    def input_positions(self) -> List[int]:
        return self.input_pos[: self.num_sequences].tolist()

    @property
    def is_generate(self) -> bool:
        return all(sl == 1 for sl in self.sequence_lengths)

    @property
    def num_pages(self) -> int:
        return self._num_pages

    @num_pages.setter
    def num_pages(self, value):
        self._num_pages = value
        # update the cache_loc tensor
        self.cache_loc.resize_(value)

    @property
    def is_paged(self) -> bool:
        return self.page_size < self.max_seq_len

    @property
    def page_assignments(self) -> List[List[int]]:
        """Return the page assignments for each sequence."""
        pages_per_seq = self.pages_per_seq[: self.num_sequences].tolist()
        return [
            c_loc_one_seq.tolist()
            for c_loc_one_seq in torch.split(self.cache_loc[: sum(pages_per_seq)], pages_per_seq)
        ]

    @classmethod
    def _get_sanitized_seq_len(cls, input_ids: torch.Tensor, seq_len: torch.Tensor) -> torch.Tensor:
        """Sanitize sequence lengths.

        We want to cover the following scenarios with this function:

        1. Pre-fill:
            input_ids: [1, s_total, ...]
            seq_len: [s_0, s_1, ..., s_{b-1}, 0, 0, ..., 0]
            ---> returns [s_0, s_1, ..., s_{b-1}]
        2. Decode:
            input_ids: [b, 1, ...]
            seq_len: [1, 1, ..., 1, 0, 0, ..., ..., ..., ..., 0]
                     |---- b ----|--- (max_batch_size - b) ---|
            --> returns [1,] * b
        3. Decode in Cudagraph:
            input_ids: [b_cudagraph, 1, ...]
            seq_len: [1, 1, ..., 1, 0, 0, ..., ..., ..., ..., 0]
                     |---- b ----|--- (max_batch_size - b) ---|

            --> returns [1,] * b_cudagraph
            Here b <= b_cudagraph. We want to make sure that the seq_len is one-padded to
            b_cudagraph.

            # TODO: I could see one possible issue with this approach in the future.
            # If we have b < b_cudagraph we now one-pad. However, we don't pad the cache location
            # information. What could happen is that the for the padded sequences the cache location
            # tensors point to allocated pages. This could lead to a situation where we write into
            # allocated cache pages polluting the cache of other sequences. Now this is not an issue
            # if we write the dummy sequences into unallocated cache pages... One fix could be to
            # pad not only the seq len but also pad the cache locations by just repeating the last
            # valid cache location in the batch. This would ensure that the dummy sequences just
            # repeats valid computation...
        """
        _, s = input_ids.shape[:2]
        num_seq = cls._get_sanitized_num_sequences(input_ids, seq_len)
        if s > 1:
            return seq_len[:num_seq].detach().clone()
        else:
            return torch.ones(num_seq, dtype=seq_len.dtype, device=seq_len.device)

    @staticmethod
    def _get_sanitized_num_sequences(input_ids: torch.Tensor, seq_len: torch.Tensor) -> int:
        """Get number of sequences.

        We makes sure that this function is compatible with both torch graph capture and cudagraph.
        Both can be a bit temparamental when trying to extract the number of sequences from a tensor
        with max_batch_size or max_batch_size*max_seq_len.
        """
        b, s = input_ids.shape[:2]
        if s > 1:
            num_seq = torch.sum(seq_len > 0)
            assert seq_len[num_seq:].sum() == 0, "seq_len should be zero-padded"
        else:
            num_seq = b
        return num_seq

    def switch_to_cached_attn_inputs(self) -> List[str]:
        """Switch to inputs for cached+flattened attention operators.

        Returns:
            List[str]: List of new argument names that are now activated.

        This function will change the inputs provided by the interface from the arguments expected
        by regular attention in PyTorch (SDPA-style) to the arguments needed once we use attention
        operators with cache support and flattened sequences.

        NOTE: The graph inference optimizer is responsible for ensuring the the new inputs are
        correctly reflected in the graph after this function is called.
        """
        assert not self._is_cached_attn, "Cached+flattened attention already activated"
        self._is_cached_attn = True
        return self._cached_arg_names.copy()

    def to(self, *args, **kwargs) -> None:
        for k in self._uncached_arg_names + self._cached_arg_names:
            setattr(self, k, getattr(self, k).to(*args, **kwargs))

        for k, v in self._extra_args.items():
            if isinstance(v, torch.Tensor):
                self._extra_args[k] = v.to(*args, **kwargs)

    def reset(self) -> None:
        """Reset the sequence information.

        After reset the sequence information should correspond to a "generate-only" batch of
        sequences (b, s==1) without cache history.
        """
        # reset input_pos
        self.input_pos.zero_()

        # set a dummy sequence corresponding to a generate-only batch (will also reset position_ids)
        self.nest_sequences(torch.zeros(self.max_batch_size, 1, dtype=torch.int))

        # reset cache information
        self.cache_loc[:] = torch.arange(self.num_pages, dtype=torch.int, device=self.device)
        self.pages_per_seq.fill_(1)

    def set_example_sequence(self) -> None:
        """Set an example sequence useful for testing and export purposes."""
        self.reset()
        bs, seq_len = min(2, self.max_batch_size), min(4, self.max_seq_len)
        input_ids = torch.ones(
            bs,
            seq_len,
            dtype=torch.int,
            device=self.device,
        )
        self.nest_sequences(input_ids)

    def set_max_num_tokens_sample(self) -> None:
        """Set an example sequence with max_num_tokens."""
        self.reset()
        seq_len = self.max_num_tokens // self.max_batch_size
        input_ids = torch.ones(
            self.max_batch_size,
            seq_len,
            dtype=torch.int,
            device=self.device,
        )
        self.pages_per_seq.fill_(seq_len // self.page_size)
        self.nest_sequences(input_ids)

    def set_generate_only_batch(self) -> None:
        """Set an example sequence for generate-only batch.

        NOTE: this batch is already formatted as [b, 1] in both original and in the cached attention
        mode. So we don't need to do anything mode-specific here.
        """
        self.reset()
        self.nest_sequences([[1]] * self.max_batch_size)

    def _update_position_ids(self) -> None:
        # set new position_ids as new tensor from input_pos and seq_len via torch.arange
        position_ids_list = [
            num
            for in_pos, seq_len in zip(self.input_positions, self.sequence_lengths)
            for num in range(in_pos, in_pos + seq_len)
        ]
        self.position_ids = torch.tensor(position_ids_list, dtype=torch.long).to(self.device)

        # use [b,1] shape to indicate generate-only batch, otherwise use [1,total_len]
        if self.is_generate:
            self.position_ids = self.position_ids.view(-1, 1)
        else:
            self.position_ids = self.position_ids.view(1, -1)

    def _generate_position_ids(self) -> torch.Tensor:
        """Generate position ids from current input_pos and sequence lengths."""
        position_ids_list = [
            num
            for in_pos, seq_len in zip(self.input_positions, self.sequence_lengths)
            for num in range(in_pos, in_pos + seq_len)
        ]
        return torch.tensor(position_ids_list, dtype=torch.long).to(self.device)

    def _update_input_pos(self, seq_len: Union[torch.Tensor, List[int], int]) -> None:
        """Update the starting position for each sequence in the cache.

        If ``reset=True`, ``input_pos`` will be reset to zero before updating.
        """
        if not isinstance(seq_len, torch.Tensor):
            seq_len = torch.tensor(seq_len, dtype=torch.int)
        bs = len(seq_len) if seq_len.dim() > 0 else self.max_batch_size
        self.input_pos[:bs] = seq_len.to(self.device)

    def _assign_pages_per_seq(self, page_assignments: Sequence[Sequence[int]]) -> None:
        """Set the cache location and pages_per_seq tensors from page assignments."""
        assert len(page_assignments) == self.num_sequences
        cache_loc_flat = torch.tensor(
            [p_idx for pages in page_assignments for p_idx in pages], dtype=torch.int
        )
        self.cache_loc[: len(cache_loc_flat)].copy_(cache_loc_flat, non_blocking=True)

        pages_per_seq = torch.tensor([len(p) for p in page_assignments], dtype=torch.int)
        self.pages_per_seq[: len(pages_per_seq)].copy_(pages_per_seq, non_blocking=True)

    @staticmethod
    def _flatten(nested_seqs: Sequence[Sequence[int]]) -> List[int]:
        return [
            val
            for lst in nested_seqs
            for val in (lst.detach().tolist() if isinstance(lst, torch.Tensor) else lst)
        ]

    def _shape_for_forward(self, tnsr: torch.Tensor) -> torch.Tensor:
        """Shape the tensor for the forward pass based on the current attention mode.

        Args:
            tnsr: The tensor to shape assumed to be in shape [batch_size*seq_len, ...]

        Returns:
            The shaped tensor flattened or unflattened based on the current attention mode.
        """
        # check if we are still running uncached attention in which case we are also still
        # operate on unflattened tensors with explicit [batch_size, seq_len, ...] shape
        if not self._is_cached_attn:
            bs = len(self.sequence_lengths)
            sl = self.sequence_lengths[0]
            return tnsr.view(bs, sl, *tnsr.shape[2:])

        # use [b,1] shape to indicate generate-only batch, otherwise use [1,total_len]
        if self.is_generate:
            return tnsr.view(-1, 1, *tnsr.shape[1:])
        else:
            return tnsr.view(1, -1, *tnsr.shape[1:])

    def nest_sequences(
        self,
        input_ids: Sequence[Sequence[int]],
        position_ids: Optional[Sequence[Sequence[int]]] = None,
        input_pos: Optional[Union[torch.Tensor, Sequence[int], int]] = None,
        page_assignments: Optional[Sequence[Sequence[int]]] = None,
    ) -> None:
        """Create and store a flattened list of input_ids from the provided list of sequences.

        Args:
            input_ids: List of sequences of input_ids.
            position_ids: List of sequences of position_ids for each token.
            input_pos: Absolute starting position in the cache for each sequence.
            page_assignments: List of sequences of page assignments for each sequence.

        This i/f will ensure that all sequence info args are updated accordingly.
        """

        # set new sequence lengths
        seq_lens = [len(ids) for ids in input_ids]
        self.seq_len.zero_()
        self.seq_len[: len(seq_lens)].copy_(torch.tensor(seq_lens), non_blocking=True)
        self._sequence_lengths = seq_lens

        # We'll preserve the dtype of the input_ids tensor if it is a tensor, otherwise we'll use int
        dtype = input_ids.dtype if isinstance(input_ids, torch.Tensor) else torch.int

        # set new input_ids as new tensor from flattened input_ids
        self.input_ids = torch.tensor(self._flatten(input_ids), dtype=dtype).to(self.device)
        self.input_ids = self._shape_for_forward(self.input_ids)

        # check for position_ids/input_pos update
        assert position_ids is None or input_pos is None, (
            "Cannot provide both position_ids and input_pos"
        )
        # check for updated input_pos
        if input_pos is not None:
            self._update_input_pos(input_pos)

        # check for updated position_ids
        if position_ids is None:
            # none provided,simple update position_ids based on new sequence lengths and
            # current input_pos assuming that input_pos is the starting position id for each
            # sequence and position_ids are consecutive.
            self.position_ids = self._generate_position_ids()
        elif not isinstance(position_ids, torch.Tensor):
            # nest position_ids to be consistent with input_ids
            seq_lens_p = [len(ids) for ids in position_ids]
            assert len(seq_lens_p) == len(seq_lens), f"{seq_lens_p=} != {seq_lens=}"
            position_ids_flat = self._flatten(position_ids)
            self.position_ids = torch.tensor(
                position_ids_flat, dtype=torch.long, device=self.device
            )
        else:
            self.position_ids = position_ids

        # final shape for position_ids
        self.position_ids = self._shape_for_forward(self.position_ids)

        # sanity check on final shape of position_ids and input_ids
        assert self.position_ids.shape[:2] == self.input_ids.shape[:2], (
            f"{self.position_ids.shape[:2]=} != {self.input_ids.shape[:2]=}"
        )

        # check for updated page_assignments
        if page_assignments is not None:
            self._assign_pages_per_seq(page_assignments)

    def unnest_sequences(self, t_nested: torch.Tensor) -> List[torch.Tensor]:
        t_squeezed = t_nested.squeeze(1) if self.is_generate else t_nested.squeeze(0)
        return list(torch.split(t_squeezed, self.sequence_lengths))

    def add_extra_arg(
        self,
        name: str,
        value: torch.Tensor,
        dynamic_shape_callback: Optional[DynamicShapeCallback] = None,
    ) -> None:
        """Add an extra argument to the sequence info object.

        Args:
            name: The name of the extra argument.
            value: Example input value of the extra argument.
            dynamic_shape_callback: The callback to get the dynamic shape of the extra argument.

        Note that the extra argument is expected to be a tensor.
        """
        self._extra_args[name] = value.to(self.device)
        if dynamic_shape_callback is None:
            self._extra_dynamic_shapes_callbacks[name] = lambda: {}
        else:
            self._extra_dynamic_shapes_callbacks[name] = dynamic_shape_callback

    def set_extra_arg(self, name: str, value: torch.Tensor) -> None:
        """Set an extra argument to the sequence info."""
        # TODO (lucaslie): assume fixed shape for now
        self._extra_args[name].copy_(value.to(self.device), non_blocking=True)


Constant = Union[int, float, str, None]


class MHACallable(Protocol):
    def __call__(
        self,
        *qkv_metadata_and_caches: Union[torch.Tensor, Constant],
    ) -> torch.Tensor: ...


class PrepareMetadataCallable(Protocol):
    def __call__(
        self,
        input_ids: torch.Tensor,
        position_ids: torch.Tensor,
        seq_len: torch.Tensor,
        input_pos: torch.Tensor,
        cache_loc: torch.Tensor,
        pages_per_seq: torch.Tensor,
        page_size: int,
    ) -> List[torch.Tensor]: ...


class GetCacheCallable(Protocol):
    def __call__(self, sequence_info: SequenceInfo) -> torch.Tensor: ...


class GetBufferCallable(GetCacheCallable):
    pass


CacheInitializerDict = Dict[str, GetCacheCallable]
BufferInitializerDict = Dict[str, GetBufferCallable]
AttentionLayout = Literal["bsnd", "bnsd"]


class AttentionDescriptor(ABC):
    """An interface to define a functional attention operator.

    The main logic is contained with the actual attention op as well as the prepare_metadata op. The
    prepare_metadata op is responsible for converting the standardized sequence info into metadata
    specific to the attention op.
    """

    @classmethod
    @abstractmethod
    def is_paged(cls) -> bool:
        """Return if the attention op is paged or not."""

    @classmethod
    @abstractmethod
    def get_attention_layout(cls) -> AttentionLayout:
        """Get the attention layout expected by the source op and the cached attention op."""

    @classmethod
    @abstractmethod
    def get_num_qkv_args(cls) -> int:
        """Get the number of qkv arguments expected by the source op."""

    @classmethod
    @abstractmethod
    def get_source_attention_op(cls) -> OpOverloadPacket:
        """Get the source attention op that we target for replacement."""

    @classmethod
    @abstractmethod
    def get_cached_attention_op(cls) -> MHACallable:
        """Get the cached attention op .

        The attention_op should follow the below signature:

        ```
        def attention_op(
            *qkv,       # list of tensors corresponding to Q, K, V as in source attention op
            *metadata,  # global info about the sequences as returned by the prepare_metadata op
            *caches,    # contains layer-specific caches per provided cache initializers
            *buffers,   # global buffers used by the attention op as provided by buffer initializers
            *constants, # basic arguments (int, float, str, None) added as CONSTANTS in the graph
        ) -> torch.Tensor: ...
        ```

        **Note that the attention op should be a valid torch custom op, which comes with
        restrictions on the supported types in the signature.**

        **Note that the `qkv` tuple should be consistent across both the cached attention
        op and the source attention op that it is replacing.**

        """
        raise NotImplementedError

    @classmethod
    @abstractmethod
    def get_prepare_metadata_op(cls) -> Tuple[PrepareMetadataCallable, int]:
        """Get the prepare_metadata op.

        The prepare_metadata op should follow the below signature:

        ```
        def prepare_metadata(
            input_ids: torch.Tensor,
            position_ids: torch.Tensor,
            seq_len: torch.Tensor,
            input_pos: torch.Tensor,
            cache_loc: torch.Tensor,
        ) -> List[torch.Tensor]: ...
        ```
        The metadata should contain all necessary global information required for the underlying
        attention op to process the input sequence and the returned list of tensors will be passed
        on to each invocation of the attention op in the graph.

        prepare_metadata is called once at the beginning of the forward pass.

        **Note that the prepare_metadata op should be a valid torch custom op, which comes with
        restrictions on the supported types in the signature.**
        """

    @classmethod
    @abstractmethod
    def get_cache_initializers(
        cls, source_attn_node: Node, cache_config: CacheConfig
    ) -> CacheInitializerDict:
        """Provide a dictionary of function pointers that can be used to initialize the caches.

        The key corresponds to the argument name used in the attention op signature. The function
        key doesn't need to be unique across multiple attention nodes in the graph. The key used to
        describe the cache in the graph will be patched with the attention node index to ensure
        uniqueness.

        ``get_cache_initializers`` will be called *once* during cache initialization and before
        the initial forward pass for each attention op detected in the graph. The caches will be
        managed by the global CacheManager and passed back to the attention op during the forward
        pass.

        If the cache initializer requires information about the attention op, it can retrieve
        the necessary information from the source attention node and cache config.
        """

    @classmethod
    def get_global_buffer_initializers(cls, source_attn_node: Node) -> BufferInitializerDict:
        """Provide a dictionary of function pointers that can be used to initialize buffers.

        The key corresponds to the buffer name used in the graph module and will **not**
        be patched unlike a cache key. Hence, it is a **global** key that is shared across all
        attention ops in the model much like a regular buffer in an nn.Module. That means if this
        i/f is called for multiple attention ops, the same buffer will be shared across all of them
        if this function provides the same key multiple times.

        Buffers are initialize *once* after the model initialization and before the initial forward
        pass for each attention op detected in the graph. The buffer will be managed by the global
        CacheManager and passed back to the attention op during the forward pass.

        If the buffer initializer requires information about the attention op, it can retrieve
        the necessary information from the source attention node.
        """

    @classmethod
    @abstractmethod
    def get_constants(cls, source_attn_node: Node) -> List[Constant]:
        """Provide a list of constant arguments to be passed to the attention op.

        The constant arguments are passed to the attention op as additional arguments after the
        caches and buffers. The constants are expected to be of type int, float, str, or None.
        """


class AttentionRegistry:
    """A simple registry to look up different attention implementations."""

    _attention_registry: Dict[str, Type["AttentionDescriptor"]] = {}

    @classmethod
    def register(cls, kernel_source: str) -> Type["AttentionDescriptor"]:
        def decorator(attention_cls: Type["AttentionDescriptor"]):
            assert kernel_source not in cls._attention_registry, (
                f"Attention source {kernel_source} already registered."
            )
            cls._attention_registry[kernel_source] = attention_cls
            return attention_cls

        return decorator

    @classmethod
    def get(cls, kernel_source: str) -> Type["AttentionDescriptor"]:
        assert cls.has(kernel_source), f"Attention source {kernel_source} not registered."
        return cls._attention_registry[kernel_source]

    @classmethod
    def has(cls, kernel_source: str) -> bool:
        return kernel_source in cls._attention_registry
